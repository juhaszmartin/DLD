{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae858fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 files in data\n",
      "\n",
      "================================================================================\n",
      "AdjustedWPsize.json  (10307 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=20548495463.0\n",
      "  key='Unknown': type=float val=1029061.1063966773\n",
      "  key='ann': type=float val=736835.9771795318\n",
      "  key='atj': type=float val=1233541.7363999982\n",
      "  key='ady': type=float val=661722.7910896443\n",
      "\n",
      "================================================================================\n",
      "Articles.json  (6205 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=int val=4978181\n",
      "  key='Unknown': type=int val=846\n",
      "  key='ann': type=int val=507\n",
      "  key='atj': type=int val=2286\n",
      "  key='ady': type=int val=1042\n",
      "\n",
      "================================================================================\n",
      "Avggoodpagelength.json  (10356 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=4157.484884940905\n",
      "  key='Unknown': type=float val=1298.330203289527\n",
      "  key='ann': type=float val=1516.7147731933603\n",
      "  key='atj': type=float val=652.7266448641674\n",
      "  key='ady': type=float val=711.6824809082915\n",
      "\n",
      "================================================================================\n",
      "Realtotalratio.json  (10538 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=0.5673514080745558\n",
      "  key='Unknown': type=float val=0.4491725768321513\n",
      "  key='ann': type=float val=0.7593688362919132\n",
      "  key='atj': type=float val=0.44925634295713035\n",
      "  key='ady': type=float val=0.3675623800383877\n",
      "\n",
      "================================================================================\n",
      "WPincubatornew.json  (115188 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (7679): ['aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag', 'aah', 'aai', 'aak', 'aal', 'aan', 'aao', 'aap', 'aaq', 'aar', 'aas', 'aat', 'aau', 'aaw']\n",
      "  key='aaa': type=int val=0\n",
      "  key='aab': type=int val=0\n",
      "  key='aac': type=int val=0\n",
      "  key='aad': type=int val=0\n",
      "  key='aae': type=int val=0\n",
      "\n",
      "================================================================================\n",
      "WPsizeinchars.json  (10307 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=20548495463.0\n",
      "  key='Unknown': type=float val=1029061.1063966773\n",
      "  key='ann': type=float val=736835.9771795318\n",
      "  key='atj': type=float val=1233541.7363999982\n",
      "  key='ady': type=float val=661722.7910896443\n",
      "\n",
      "================================================================================\n",
      "data_codes.json  (84964 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level list\n",
      "List length 9012; showing up to 5 items:\n",
      "  [1] type=str preview='aaa'\n",
      "  [2] type=str preview='aab'\n",
      "  [3] type=str preview='aac'\n",
      "  [4] type=str preview='aad'\n",
      "  [5] type=str preview='aae'\n",
      "\n",
      "================================================================================\n",
      "ethnologue_code_list.txt  (38415 bytes)  suffix=.txt\n",
      "--------------------------------------------------------------------------------\n",
      "aaa\n",
      "aab\n",
      "aac\n",
      "aad\n",
      "aae\n",
      "aaf\n",
      "aag\n",
      "aah\n",
      "aai\n",
      "aak\n",
      "\n",
      "================================================================================\n",
      "ethnologue_language_data.csv  (2169855 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (9): ['ISO Code', 'Language Name', 'Summary', 'Population Size', 'Institutional (%)', 'Stable (%)', 'Endangered (%)', 'Extinct (%)', 'Digital Support']\n",
      "   ['aaa', 'Ghotuo', 'Ghotuo is a stable indigenous language of Nigeria. It belongs to the Niger-Congo language family. Direct evidence is lacking, but the language is thought to be used as a first language by all in the ethnic community. It is not known to be taught in schools.', 'Less than 10K', '0%', '100%', '0%', '0%', 'Emerging']\n",
      "   ['aab', 'Arum', 'Arum is a stable indigenous language of Nigeria. It belongs to the Niger-Congo language family. The language is used as a first language by all in the ethnic community. It is not known to be taught in schools.', '10K to 1M', '0%', '100%', '0%', '0%', 'Still']\n",
      "   ['aac', 'Ari', 'Ari is an endangered indigenous language of Papua New Guinea. It belongs to the Trans-New Guinea language family. The language is used as a first language by adults only. It is not known to be taught in schools.', 'None', '0%', '0%', '100%', '0%', 'Still']\n",
      "   ['aad', 'Amal', 'Amal is a stable indigenous language of Papua New Guinea. It belongs to the Sepik language family. Direct evidence is lacking, but the language is thought to be used as a first language by all in the ethnic community. It is not known to be taught in schools.', 'Less than 10K', '0%', '100%', '0%', '0%', 'Still']\n",
      "   ['aae', 'Arbëreshë Albanian', 'Arbëreshë Albanian is an endangered indigenous language of Italy. It belongs to the Indo-European language family and is part of the Albanian macrolanguage. The language is used as a first language by all adults in the ethnic community, but not all young people. It is not known to be taught in schools.', '10K to 1M', '0%', '0%', '100%', '0%', 'Ascending']\n",
      "\n",
      "================================================================================\n",
      "features_by_code.csv  (3878665 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (23): ['code      ', ' canonical_code', ' is_iso3', ' is_glottocode', ' Articles', ' Avggoodpagelength  ', ' adjusted_wp_size       ', ' article_count', ' avg_article_length ', ' entropy           ', ' ethnologue_population', ' glotto_mapped_iso', ' glottocode', ' jw_bible_available', ' median_article_length', ' mention_total', ' os_base_required       ', ' real_ratio          ', ' realtotalratio      ', ' wals_family          ']\n",
      "   ['ghot1243  ', ' aaa           ', ' True   ', ' True         ', '         ', '                    ', '                        ', '              ', '                    ', '                   ']\n",
      "   ['alum1246  ', ' aab           ', ' True   ', ' True         ', '         ', '                    ', '                        ', '              ', '                    ', '                   ']\n",
      "   ['arii1243  ', ' aac           ', ' True   ', ' True         ', '         ', '                    ', '                        ', '              ', '                    ', '                   ']\n",
      "   ['amal1242  ', ' aad           ', ' True   ', ' True         ', '         ', '                    ', '                        ', '              ', '                    ', '                   ']\n",
      "   ['arbe1236  ', ' aae           ', ' True   ', ' True         ', '         ', '                    ', '                        ', '              ', '                    ', '                   ']\n",
      "\n",
      "================================================================================\n",
      "glottolog_languages.csv  (522218 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (8): ['Glottocode', 'Name', 'Top-level family', 'ISO-639-3', 'Macro-area', 'Child dialects', 'Latitude', 'Longitude']\n",
      "   ['aari1239', 'Aari', 'South Omotic', 'aiw', 'Africa', '0', '5.95', '36.57']\n",
      "   ['aari1240', 'Aariya', 'Bookkeeping', 'aay', 'Eurasia', '0', '', '']\n",
      "   ['aasa1238', 'Aasax', 'Afro-Asiatic', 'aas', 'Africa', '0', '-4.01', '36.86']\n",
      "   ['abad1241', 'Abadi', 'Austronesian', 'kbt', 'Papunesia', '0', '-9.03', '146.99']\n",
      "   ['abag1245', 'Abaga', 'Nuclear Trans New Guinea', 'abg', 'Papunesia', '0', '-6.12', '145.66']\n",
      "\n",
      "================================================================================\n",
      "iso-639-3.tab  (178264 bytes)  suffix=.tab\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: '\t' (using csv reader)\n",
      "Header cols (8): ['Id', 'Part2b', 'Part2t', 'Part1', 'Scope', 'Language_Type', 'Ref_Name', 'Comment']\n",
      "   ['aaa', '', '', '', 'I', 'L', 'Ghotuo', '']\n",
      "   ['aab', '', '', '', 'I', 'L', 'Alumu-Tesu', '']\n",
      "   ['aac', '', '', '', 'I', 'L', 'Ari', '']\n",
      "   ['aad', '', '', '', 'I', 'L', 'Amal', '']\n",
      "   ['aae', '', '', '', 'I', 'L', 'Arbëreshë Albanian', '']\n",
      "\n",
      "================================================================================\n",
      "iso_639_1_2_codes.csv  (23692 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (5): ['ISO 639-2 Code', 'ISO 639-1 Code', 'English name of Language', 'French name of Language', 'German name of Language']\n",
      "   ['aar', 'aa', 'Afar', 'afar', 'Danakil-Sprache']\n",
      "   ['abk', 'ab', 'Abkhazian', 'abkhaze', 'Abchasisch']\n",
      "   ['ace', '', 'Achinese', 'aceh', 'Aceh-Sprache']\n",
      "   ['ach', '', 'Acoli', 'acoli', 'Acholi-Sprache']\n",
      "   ['ada', '', 'Adangme', 'adangme', 'Adangme-Sprache']\n",
      "\n",
      "================================================================================\n",
      "iso_adjusted_wikipedia_sizes.json  (10307 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=20548495463.0\n",
      "  key='Unknown': type=float val=1029061.1063966773\n",
      "  key='ann': type=float val=736835.9771795318\n",
      "  key='atj': type=float val=1233541.7363999982\n",
      "  key='ady': type=float val=661722.7910896443\n",
      "\n",
      "================================================================================\n",
      "iso_article_counts.json  (6205 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=int val=4978181\n",
      "  key='Unknown': type=int val=846\n",
      "  key='ann': type=int val=507\n",
      "  key='atj': type=int val=2286\n",
      "  key='ady': type=int val=1042\n",
      "\n",
      "================================================================================\n",
      "iso_avg_article_lengths.json  (10356 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=4157.484884940905\n",
      "  key='Unknown': type=float val=1298.330203289527\n",
      "  key='ann': type=float val=1516.7147731933603\n",
      "  key='atj': type=float val=652.7266448641674\n",
      "  key='ady': type=float val=711.6824809082915\n",
      "\n",
      "================================================================================\n",
      "iso_codes_of_wikis.txt  (8235 bytes)  suffix=.txt\n",
      "--------------------------------------------------------------------------------\n",
      "English (en): eng\n",
      "Cebuano (ceb): ceb\n",
      "Deutsch (de): deu\n",
      "français (fr): fra\n",
      "svenska (sv): swe\n",
      "Nederlands (nl): nld\n",
      "русский (ru): rus\n",
      "español (es): spa\n",
      "italiano (it): ita\n",
      "polski (pl): pol\n",
      "\n",
      "================================================================================\n",
      "iso_entropy_values.json  (10201 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=5.4014510772874615\n",
      "  key='Unknown': type=float val=5.762171490662889\n",
      "  key='ann': type=float val=5.14255638384005\n",
      "  key='atj': type=float val=4.78188493740518\n",
      "  key='ady': type=float val=5.9934533070342555\n",
      "\n",
      "================================================================================\n",
      "iso_median_article_lengths.json  (10317 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=1666.0\n",
      "  key='Unknown': type=float val=372.14721588170397\n",
      "  key='ann': type=float val=850.7782989704066\n",
      "  key='atj': type=float val=368.80305566100344\n",
      "  key='ady': type=float val=218.54710784263443\n",
      "\n",
      "================================================================================\n",
      "iso_real_ratios.json  (10538 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (329): ['deu', 'Unknown', 'ann', 'atj', 'ady', 'arc', 'ami', 'anp', 'awa', 'alt', 'aym', 'ang', 'bbc', 'ace', 'bdr', 'abk', 'ava', 'bew', 'bis', 'amh']\n",
      "  key='deu': type=float val=0.5673514080745558\n",
      "  key='Unknown': type=float val=0.4491725768321513\n",
      "  key='ann': type=float val=0.7593688362919132\n",
      "  key='atj': type=float val=0.44925634295713035\n",
      "  key='ady': type=float val=0.3675623800383877\n",
      "\n",
      "================================================================================\n",
      "jw_availability_by_iso.csv  (36235 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (8): ['ISO 639-2 Code', 'ISO 639-1 Code', 'English name of Language', 'French name of Language', 'German name of Language', 'JW_for_iso1_status', 'JW_for_iso2_status', 'Does_have_bible']\n",
      "   ['aar', 'aa', 'Afar', 'afar', 'Danakil-Sprache', 'Redirected to EN', 'Redirected to EN', '0']\n",
      "   ['abk', 'ab', 'Abkhazian', 'abkhaze', 'Abchasisch', 'Exists', 'Exists', '1']\n",
      "   ['ace', '', 'Achinese', 'aceh', 'Aceh-Sprache', 'N/A', 'Redirected to EN', '0']\n",
      "   ['ach', '', 'Acoli', 'acoli', 'Acholi-Sprache', 'N/A', 'Exists', '1']\n",
      "   ['ada', '', 'Adangme', 'adangme', 'Adangme-Sprache', 'N/A', 'Exists', '1']\n",
      "\n",
      "================================================================================\n",
      "jw_languages.csv  (25266 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (2): ['English Name', 'Local Name']\n",
      "   ['Abaknon', 'Abaknon']\n",
      "   ['Abbey', 'Abɛ']\n",
      "   ['Abkhaz', 'аԥсуа']\n",
      "   ['Abua', 'Abua']\n",
      "   ['Abui', 'Alor Abui']\n",
      "\n",
      "================================================================================\n",
      "language_mention_matrix.csv  (221043 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (331): ['ISO_Code', 'Unknown', 'abk', 'ace', 'ady', 'afr', 'als', 'alt', 'amh', 'ami', 'ang', 'ann', 'anp', 'ara', 'arc', 'arg', 'ary', 'arz', 'asm', 'ast']\n",
      "   ['Unknown', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "   ['abk', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "   ['ace', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "   ['ady', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "   ['afr', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "\n",
      "================================================================================\n",
      "os_support_windows.csv  (4041 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (4): ['Language', 'Native name', 'Base language required', 'ISO 639-3 Code']\n",
      "   ['Afrikaans', 'Afrikaans', 'English (United States)', 'afr']\n",
      "   ['Albanian', 'shqip', 'English (United States)', 'sqi']\n",
      "   ['Amharic', 'አማርኛ', 'English (United States)', 'amh']\n",
      "   ['Arabic', 'العربية', 'Any language', 'ara']\n",
      "   ['Armenian', 'Հայերեն', 'English (United States)', 'hye']\n",
      "\n",
      "================================================================================\n",
      "tatoeba_sentences_by_language.csv  (8316 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (3): ['ISO 639-3', 'Language', 'Sentences']\n",
      "   ['eng', 'English', '2,005,284']\n",
      "   ['rus', 'Russian', '1,162,342']\n",
      "   ['ita', 'Italian', '947,270']\n",
      "   ['epo', 'Esperanto', '800,235']\n",
      "   ['kab', 'Kabyle', '774,864']\n",
      "\n",
      "================================================================================\n",
      "wals_languages.csv  (199220 bytes)  suffix=.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Detected delimiter: ',' (using csv reader)\n",
      "Header cols (9): ['Name', 'WALS code', 'ISO 639-3', 'Genus', 'Family', 'Macroarea', 'Latitude', 'Longitude', 'Countries']\n",
      "   ['Aari', 'aar', 'aiw', 'South Omotic', 'Afro-Asiatic', 'Africa', '6.00', '36.58', 'Ethiopia']\n",
      "   ['Abau', 'aba', 'aau', 'Abau', 'Sepik', 'Papunesia', '-4.00', '141.25', 'Papua New Guinea']\n",
      "   ['Abaza', 'abz', 'abq', 'Northwest Caucasian', 'Northwest Caucasian', 'Eurasia', '44.00', '42.00', 'Russia']\n",
      "   ['Abenaki (Western)', 'abw', 'abe', 'Algonquian', 'Algic', 'North America', '44.00', '-72.25', 'CanadaUnited States']\n",
      "   ['Abidji', 'abd', 'abi', 'Agneby', 'Niger-Congo', 'Africa', '5.67', '-4.58', \"Côte d'Ivoire\"]\n",
      "\n",
      "================================================================================\n",
      "wiki_code_to_iso_code.json  (6118 bytes)  suffix=.json\n",
      "--------------------------------------------------------------------------------\n",
      "JSON parsed: top-level dict\n",
      "Top-level keys (343): ['en', 'ceb', 'de', 'fr', 'sv', 'nl', 'ru', 'es', 'it', 'pl', 'arz', 'zh', 'ja', 'uk', 'vi', 'ar', 'war', 'pt', 'fa', 'ca']\n",
      "  key='en': type=str val='eng'\n",
      "  key='ceb': type=str val='ceb'\n",
      "  key='de': type=str val='deu'\n",
      "  key='fr': type=str val='fra'\n",
      "  key='sv': type=str val='swe'\n"
     ]
    }
   ],
   "source": [
    "import json, csv, sys\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "MAX_LINES = 10\n",
    "\n",
    "def print_hdr(s):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(s)\n",
    "    print(\"-\"*80)\n",
    "\n",
    "def inspect_json(p):\n",
    "    txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    # try JSONL first (many small JSON objects per line)\n",
    "    lines = txt.splitlines()\n",
    "    if len(lines) > 1:\n",
    "        parsed_some = []\n",
    "        for i, ln in enumerate(lines[:MAX_LINES]):\n",
    "            ln_strip = ln.strip()\n",
    "            if not ln_strip:\n",
    "                continue\n",
    "            try:\n",
    "                parsed_some.append(json.loads(ln_strip))\n",
    "            except Exception:\n",
    "                break\n",
    "        if parsed_some and len(parsed_some) > 0 and len(parsed_some) <= MAX_LINES:\n",
    "            print(f\"JSONL-like: {len(lines)} lines, showing first {len(parsed_some)} parsed objects\")\n",
    "            for i, obj in enumerate(parsed_some, 1):\n",
    "                print(f\"  [{i}] type={type(obj).__name__} preview: \", end=\"\")\n",
    "                if isinstance(obj, dict):\n",
    "                    print(\", \".join(list(obj.keys())[:10]))\n",
    "                else:\n",
    "                    print(repr(obj)[:200])\n",
    "            return\n",
    "    # fall back to canonical JSON\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        print(f\"JSON parsed: top-level {type(obj).__name__}\")\n",
    "        if isinstance(obj, dict):\n",
    "            keys = list(obj.keys())\n",
    "            print(f\"Top-level keys ({len(keys)}): {keys[:20]}\")\n",
    "            # sample up to 5 values for inspection\n",
    "            for k in keys[:5]:\n",
    "                v = obj[k]\n",
    "                print(f\"  key='{k}': type={type(v).__name__} \", end=\"\")\n",
    "                if isinstance(v, (list, dict)):\n",
    "                    print(f\"len={len(v)} sample_keys_or_first_elems=\", end=\"\")\n",
    "                    if isinstance(v, dict):\n",
    "                        print(list(v.keys())[:5])\n",
    "                    else:\n",
    "                        print(v[:3])\n",
    "                else:\n",
    "                    tv = repr(v)[:200]\n",
    "                    print(f\"val={tv}\")\n",
    "        elif isinstance(obj, list):\n",
    "            print(f\"List length {len(obj)}; showing up to {min(len(obj),5)} items:\")\n",
    "            for i, item in enumerate(obj[:5], 1):\n",
    "                print(f\"  [{i}] type={type(item).__name__} preview=\", end=\"\")\n",
    "                if isinstance(item, dict):\n",
    "                    print(\", \".join(list(item.keys())[:10]))\n",
    "                else:\n",
    "                    print(repr(item)[:200])\n",
    "        else:\n",
    "            print(\"Primitive JSON value:\", repr(obj)[:200])\n",
    "    except Exception as e:\n",
    "        print(\"JSON parse failed:\", e)\n",
    "        # show head of file\n",
    "        for ln in lines[:MAX_LINES]:\n",
    "            print(ln.rstrip())\n",
    "\n",
    "def inspect_csv(p):\n",
    "    with p.open(encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "        sample = \"\".join([next(fh) for _ in range(50)])\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample)\n",
    "        delim = dialect.delimiter\n",
    "    except Exception:\n",
    "        delim = ','\n",
    "    print(f\"Detected delimiter: '{delim}' (using csv reader)\")\n",
    "    with p.open(encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "        reader = csv.reader(fh, delimiter=delim)\n",
    "        rows = []\n",
    "        for i, row in enumerate(reader):\n",
    "            rows.append(row)\n",
    "            if i >= 5: break\n",
    "    if rows:\n",
    "        header = rows[0]\n",
    "        print(f\"Header cols ({len(header)}): {header[:20]}\")\n",
    "        for r in rows[1:]:\n",
    "            print(\"  \", r[:10])\n",
    "    else:\n",
    "        print(\"No rows read (empty file?)\")\n",
    "\n",
    "def inspect_text(p):\n",
    "    with p.open(encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "        for i, ln in enumerate(fh):\n",
    "            if i >= MAX_LINES: break\n",
    "            print(ln.rstrip())\n",
    "\n",
    "def main():\n",
    "    if not DATA_DIR.exists():\n",
    "        print(\"data dir not found:\", DATA_DIR)\n",
    "        sys.exit(1)\n",
    "    files = sorted([p for p in DATA_DIR.iterdir() if p.is_file()])\n",
    "    print(f\"Found {len(files)} files in {DATA_DIR}\")\n",
    "    for p in files:\n",
    "        print_hdr(f\"{p.name}  ({p.stat().st_size} bytes)  suffix={p.suffix}\")\n",
    "        try:\n",
    "            if p.suffix.lower() in (\".json\",):\n",
    "                inspect_json(p)\n",
    "            elif p.suffix.lower() in (\".csv\", \".tsv\") or p.name.endswith(\".tab\"):\n",
    "                inspect_csv(p)\n",
    "            else:\n",
    "                # txt, .tab, .txt-like\n",
    "                inspect_text(p)\n",
    "        except StopIteration:\n",
    "            print(\"[file shorter than expected]\")\n",
    "        except Exception as e:\n",
    "            print(\"Error inspecting file:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a11abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 16572 codes -> data/data_codes.json\n",
      "Wrote DataFrame -> data/master_features_by_code.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "import json, csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUT_CSV = DATA_DIR / \"master_features_by_code.csv\"\n",
    "OUT_CODES = DATA_DIR / \"data_codes.json\"\n",
    "\n",
    "ISO_TAB = DATA_DIR / \"iso-639-3.tab\"\n",
    "GLOTTO_CSV = DATA_DIR / \"glottolog_languages.csv\"\n",
    "WIKI_JSON = DATA_DIR / \"wiki_code_to_iso_code.json\"\n",
    "\n",
    "JSON_FEATURES = {\n",
    "    \"adjustedwpsize\": DATA_DIR / \"iso_adjusted_wikipedia_sizes.json\",\n",
    "    \"articles\": DATA_DIR / \"Articles.json\",\n",
    "    \"wpincubatornew\": DATA_DIR / \"WPincubatornew.json\",\n",
    "    \"wpsizeinchars\": DATA_DIR / \"WPsizeinchars.json\",\n",
    "    \"realtotalratio\": DATA_DIR / \"Realtotalratio.json\",\n",
    "    \"avggoodpagelength\": DATA_DIR / \"Avggoodpagelength.json\",\n",
    "}\n",
    "\n",
    "ETHNO_CSV = DATA_DIR / \"ethnologue_language_data.csv\"\n",
    "JW_CSV = DATA_DIR / \"jw_availability_by_iso.csv\"\n",
    "OS_CSV = DATA_DIR / \"os_support_windows.csv\"\n",
    "TATOEBA_CSV = DATA_DIR / \"tatoeba_sentences_by_language.csv\"\n",
    "WALS_CSV = DATA_DIR / \"wals_languages.csv\"\n",
    "\n",
    "\n",
    "def read_json(p):\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def load_iso_tab(p=ISO_TAB):\n",
    "    iso_set = set(); iso2map = {}\n",
    "    if not p.exists(): return iso_set, iso2map\n",
    "    with p.open(encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "        rdr = csv.reader(fh, delimiter=\"\\t\")\n",
    "        next(rdr, None)\n",
    "        for row in rdr:\n",
    "            if not row: continue\n",
    "            iso3 = row[0].strip().lower()\n",
    "            iso_set.add(iso3)\n",
    "            for part in (row[1:4] if len(row) >= 4 else row[1:]):\n",
    "                if part:\n",
    "                    iso2map[part.strip().lower()] = iso3\n",
    "    return iso_set, iso2map\n",
    "\n",
    "\n",
    "def load_glottolog(p=GLOTTO_CSV):\n",
    "    glotto_to_iso = {}; iso_to_glotto = defaultdict(list)\n",
    "    if not p.exists(): return glotto_to_iso, iso_to_glotto\n",
    "    with p.open(encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "        rdr = csv.DictReader(fh)\n",
    "        for r in rdr:\n",
    "            glt = (r.get(\"Glottocode\") or \"\").strip().lower()\n",
    "            iso = (r.get(\"ISO-639-3\") or r.get(\"ISO_639_3\") or \"\").strip().lower()\n",
    "            if glt:\n",
    "                glotto_to_iso[glt] = iso or None\n",
    "            if iso and glt:\n",
    "                iso_to_glotto[iso].append(glt)\n",
    "    return glotto_to_iso, iso_to_glotto\n",
    "\n",
    "\n",
    "def load_csv_map(p, candidates):\n",
    "    out = {}\n",
    "    if not p.exists(): return out\n",
    "    with p.open(encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "        rdr = csv.DictReader(fh)\n",
    "        fld = None\n",
    "        for c in candidates:\n",
    "            if c in (rdr.fieldnames or []):\n",
    "                fld = c; break\n",
    "        for r in rdr:\n",
    "            k = (r.get(fld) or \"\").strip().lower() if fld else \"\"\n",
    "            if k:\n",
    "                out[k] = r\n",
    "    return out\n",
    "\n",
    "\n",
    "def safe_int(s):\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).replace(\",\", \"\").strip()\n",
    "    try: return int(float(s))\n",
    "    except: return s\n",
    "\n",
    "\n",
    "# --- Load data ---\n",
    "iso_set, iso2map = load_iso_tab()\n",
    "glotto_to_iso, iso_to_glotto = load_glottolog()\n",
    "wiki_map = read_json(WIKI_JSON) if WIKI_JSON.exists() else {}\n",
    "\n",
    "json_features = {k: read_json(p) for k, p in JSON_FEATURES.items()}\n",
    "ethno_rows = load_csv_map(ETHNO_CSV, [\"ISO Code\",\"ISO_Code\"])\n",
    "wals_rows = load_csv_map(WALS_CSV, [\"ISO 639-3\",\"iso\",\"glottocode\",\"wals_code\"])\n",
    "jw_rows = load_csv_map(JW_CSV, [\"ISO 639-2 Code\",\"ISO 639-3\"])\n",
    "os_rows = load_csv_map(OS_CSV, [\"ISO 639-3 Code\",\"ISO 639-3\"])\n",
    "tatoeba_rows = load_csv_map(TATOEBA_CSV, [\"ISO 639-3\",\"glottocode\"])\n",
    "\n",
    "jw_by_iso3 = {}\n",
    "if JW_CSV.exists():\n",
    "    with JW_CSV.open(encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "        for r in csv.DictReader(fh):\n",
    "            iso2 = (r.get(\"ISO 639-2 Code\") or \"\").strip().lower()\n",
    "            val = (r.get(\"Does_have_bible\") or \"\").strip()\n",
    "            iso3 = iso2map.get(iso2)\n",
    "            if iso3: jw_by_iso3[iso3] = val\n",
    "            if iso2 and iso2 in iso_set: jw_by_iso3[iso2] = val\n",
    "\n",
    "os_set = set(os_rows.keys())\n",
    "wals_keys = set(wals_rows.keys())\n",
    "\n",
    "# --- Build master list of codes ---\n",
    "master = set(iso_set) | set(glotto_to_iso.keys())\n",
    "for d in json_features.values():\n",
    "    if isinstance(d, dict): master.update(k.lower() for k in d.keys() if isinstance(k, str))\n",
    "for m in [ethno_rows, wals_rows, os_rows, tatoeba_rows]:\n",
    "    master.update(k.lower() for k in m.keys())\n",
    "master.discard(\"\")\n",
    "\n",
    "entries = []\n",
    "for raw in sorted(master):\n",
    "    iso = glotto_to_iso.get(raw) if raw in glotto_to_iso else (raw if raw in iso_set else \"\")\n",
    "    glotto = \"\"\n",
    "    if raw in iso_to_glotto:\n",
    "        glottos = iso_to_glotto[raw]\n",
    "        if glottos:\n",
    "            glotto = glottos[0]\n",
    "    if not iso and raw in iso_set:\n",
    "        iso = raw\n",
    "    if not glotto and raw in glotto_to_iso:\n",
    "        glotto = raw\n",
    "    master_code = iso or glotto\n",
    "    entries.append((master_code, iso, glotto))\n",
    "\n",
    "\n",
    "def fetch_json(feat, iso, glotto):\n",
    "    d = json_features.get(feat) or {}\n",
    "    for key in (iso, glotto):\n",
    "        if key and key in d:\n",
    "            return d[key]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "header = [\n",
    "    \"code\", \"iso639_3\", \"glottocode\",\n",
    "    \"adjustedwpsize\",\"articles\",\"wpincubatornew\",\"wpsizeinchars\",\"realtotalratio\",\"avggoodpagelength\",\n",
    "    \"Population Size\",\"Institutional (%)\",\"Stable (%)\",\"Endangered (%)\",\"Extinct (%)\",\"Digital Support\",\n",
    "    \"has_glottolog\",\"Does_have_bible\",\"os_supported\",\"tatoeba_sentences\",\"has_wals\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for master_code, iso, glotto in entries:\n",
    "    r = {\"code\": master_code, \"iso639_3\": iso, \"glottocode\": glotto}\n",
    "    for feat in [\"adjustedwpsize\",\"articles\",\"wpincubatornew\",\"wpsizeinchars\",\"realtotalratio\",\"avggoodpagelength\"]:\n",
    "        r[feat] = fetch_json(feat, iso, glotto)\n",
    "\n",
    "    eth = ethno_rows.get(iso) or ethno_rows.get(glotto) or {}\n",
    "    r[\"Population Size\"] = eth.get(\"Population Size\", eth.get(\"Population\", \"\")) if isinstance(eth, dict) else \"\"\n",
    "    for fld in [\"Institutional (%)\",\"Stable (%)\",\"Endangered (%)\",\"Extinct (%)\",\"Digital Support\"]:\n",
    "        r[fld] = eth.get(fld, \"\") if isinstance(eth, dict) else \"\"\n",
    "\n",
    "    r[\"has_glottolog\"] = int(bool(iso_to_glotto.get(iso) or glotto_to_iso.get(glotto)))\n",
    "    r[\"Does_have_bible\"] = jw_by_iso3.get(iso, \"\")\n",
    "    r[\"os_supported\"] = 1 if (iso in os_rows or glotto in os_rows) else 0\n",
    "\n",
    "    trow = tatoeba_rows.get(iso) or tatoeba_rows.get(glotto) or {}\n",
    "    sentences = \"\"\n",
    "    if isinstance(trow, dict):\n",
    "        sentences = trow.get(\"Sentences\") or trow.get(\" Sentences\") or \"\"\n",
    "        sentences = safe_int(sentences)\n",
    "    r[\"tatoeba_sentences\"] = sentences\n",
    "\n",
    "    r[\"has_wals\"] = int(iso in wals_keys or glotto in wals_keys)\n",
    "    rows.append(r)\n",
    "\n",
    "\n",
    "# --- Save using pandas ---\n",
    "df = pd.DataFrame(rows, columns=header)\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "OUT_CODES.write_text(json.dumps(df[\"code\"].tolist(), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote {len(df)} codes -> {OUT_CODES}\")\n",
    "print(f\"Wrote DataFrame -> {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bcb1970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4848\n"
     ]
    }
   ],
   "source": [
    "count = sum(1 for row in rows if row.get(\"has_wals\") == 1 or row.get(\"has_wals\") == \"1\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6a7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
